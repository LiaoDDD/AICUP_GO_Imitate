{"cells":[{"cell_type":"code","execution_count":1,"id":"bb173248","metadata":{"executionInfo":{"elapsed":4364,"status":"ok","timestamp":1697462715895,"user":{"displayName":"Ting Yang Lu (アズーサ)","userId":"17219646350901583040"},"user_tz":-480},"id":"bb173248"},"outputs":[],"source":["import tensorflow as tf\n","from keras.models import Model\n","from keras.layers import Input, Conv2D, ReLU, Flatten, Dense, Softmax, MaxPooling2D, BatchNormalization, ELU, Dropout, LeakyReLU, GlobalAveragePooling2D, Activation\n","\n","from keras.optimizers import Adam\n","from keras import regularizers\n","import numpy as np\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"id":"ae56a295","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":2,"id":"3287d2cb","metadata":{},"outputs":[],"source":["from keras.layers import Input, Conv2D, ReLU, Flatten, Dense, Softmax, MaxPooling2D, BatchNormalization, ELU, Dropout, LeakyReLU, GlobalAveragePooling2D, Activation\n","\n","from keras.optimizers import Adam\n","from keras import regularizers\n","import numpy as np\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":3,"id":"62384345","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1697462734056,"user":{"displayName":"Ting Yang Lu (アズーサ)","userId":"17219646350901583040"},"user_tz":-480},"id":"62384345","outputId":"e6761e34-c1a0-47fe-dbd0-47c82f4cd516"},"outputs":[{"data":{"text/plain":["'2.10.0'"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["tf.__version__"]},{"cell_type":"markdown","id":"bd95ed0c","metadata":{"id":"bd95ed0c"},"source":["# Data Pre-Processing\n","\n","Open **kyu_train.csv** file and split the games into a list.\n","Every row of csv: `KL0000000001,B,B[pq],W[dd],B[dp],W[pd],B[jc],...`.\n","\n","Columns are:\n","\n","    1. KL0000000001: Game ID\n","    2. B: Player's color\n","    3-... : Moves\n","    \n","We cropped only the moves to game list as:"]},{"cell_type":"code","execution_count":4,"id":"2f8872fd","metadata":{"executionInfo":{"elapsed":3573,"status":"ok","timestamp":1697462737623,"user":{"displayName":"Ting Yang Lu (アズーサ)","userId":"17219646350901583040"},"user_tz":-480},"id":"2f8872fd"},"outputs":[],"source":["df = open('C:/Users/Tidera/Downloads/drive-download-20231017T013717Z-001/kyu_train.csv').read().splitlines()\n","games = [i.split(',',2)[-1] for i in df]"]},{"cell_type":"markdown","id":"58532b01","metadata":{"id":"58532b01"},"source":["Create a dictionary to convert the coordinates from characters to numbers"]},{"cell_type":"code","execution_count":5,"id":"496585f2","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1697462737623,"user":{"displayName":"Ting Yang Lu (アズーサ)","userId":"17219646350901583040"},"user_tz":-480},"id":"496585f2","outputId":"f4d633c0-c8dc-4dd1-9e0f-6bbfed93573e"},"outputs":[{"data":{"text/plain":["{'a': 0,\n"," 'b': 1,\n"," 'c': 2,\n"," 'd': 3,\n"," 'e': 4,\n"," 'f': 5,\n"," 'g': 6,\n"," 'h': 7,\n"," 'i': 8,\n"," 'j': 9,\n"," 'k': 10,\n"," 'l': 11,\n"," 'm': 12,\n"," 'n': 13,\n"," 'o': 14,\n"," 'p': 15,\n"," 'q': 16,\n"," 'r': 17,\n"," 's': 18}"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["chars = 'abcdefghijklmnopqrs'\n","coordinates = {k:v for v,k in enumerate(chars)}\n","chartonumbers = {k:v for k,v in enumerate(chars)}\n","coordinates"]},{"cell_type":"markdown","id":"92277370","metadata":{"id":"92277370"},"source":["We decided to build a DCNN model in this tutorial. We create data samples by using every move in every game, meaning that the target is to predict the next move by feeding the previous state of the table in every game for every move. Therefore, we can collect much more data samples from games.\n","\n","For the simplicity, we used 4 dimensional feature map to represent the data as below:\n"," 1. Positions of black stones: mark them as 1 and the rest of the table as 0\n"," 2. Positions of white stones: mark them as 1 and the rest of the table as 0\n"," 3. Empty areas of the table: mark the empty areas as 1 and occupied areas as 0\n"," 4. The last move in the table: mark the position of the last move as 1 and the rest as 0\n","\n","Target value is a number between 0-361(19\\*19). Later this will be one-hot encoded."]},{"cell_type":"code","execution_count":6,"id":"0adb423c","metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1697462737623,"user":{"displayName":"Ting Yang Lu (アズーサ)","userId":"17219646350901583040"},"user_tz":-480},"id":"0adb423c"},"outputs":[],"source":["def extract_features(moves):\n","    features = np.zeros((19, 19, 11))\n","\n","    for i, move in enumerate(moves):\n","        color = move[0]\n","        column = coordinates[move[2]]\n","        row = coordinates[move[3]]\n","\n","        features[row, column, 0] = 1 if color == 'B' else -1  # 表示下棋的顏色\n","        features[row, column, 1] = 1 if i % 2 == 0 else -1  # 表示輪到哪一方下棋\n","        features[row, column, 2] = 1  # 表示該位置上是否已下棋\n","\n","        # 最後一步的信息\n","        features[row, column, 3] = 1 if i == len(moves) - 1 else 0\n","\n","        # 最近幾步的分佈情況\n","        for j in range(1, min(5, i + 1)):\n","            prev_move = moves[i - j]\n","            prev_column = coordinates[prev_move[2]]\n","            prev_row = coordinates[prev_move[3]]\n","            features[prev_row, prev_column, j] = 1 if prev_move[0] == color else -1\n","        \n","    # 其他特徵的計算...\n","    # 這裡可以添加更多特徵的計算邏輯，例如區域控制、連子信息等\n","\n","    return features\n","\n","\n","def prepare_input(moves):\n","    x = np.zeros((19, 19, 11))\n","\n","    # 在這裡調用特徵提取函數\n","    features = extract_features(moves)\n","\n","    # 將提取的特徵放入輸入張量的特定通道\n","    x[:, :, 0:5] = features[:, :, 0:5]\n","\n","    # 其他通道的處理...\n","    # 例如，你可以在不同的通道上添加其他特徵\n","\n","    # 將你原本的邏輯放在這裡...\n","\n","        \n","\n","    if moves:\n","        last_move_column = coordinates[moves[-1][2]]\n","        last_move_row = coordinates[moves[-1][3]]\n","        x[last_move_row, last_move_column, 3] = 1\n","\n","    x[:, :, 2] = np.where(x[:, :, 2] == 0, 1, 0)\n","\n","    return x\n","\n","def prepare_label(move):\n","    column = coordinates[move[2]]\n","    row = coordinates[move[3]]\n","    return column*19+row"]},{"cell_type":"code","execution_count":7,"id":"758808ae","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4372,"status":"ok","timestamp":1697462741992,"user":{"displayName":"Ting Yang Lu (アズーサ)","userId":"17219646350901583040"},"user_tz":-480},"id":"758808ae","outputId":"93fc3526-9650-481e-f163-2506e6367071"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total Games: 118500, Total Moves: 27135638\n"]}],"source":["# Check how many samples can be obtained\n","n_games = 0\n","n_moves = 0\n","for game in games:\n","    n_games += 1\n","    moves_list = game.split(',')\n","    for move in moves_list:\n","        n_moves += 1\n","print(f\"Total Games: {n_games}, Total Moves: {n_moves}\")"]},{"cell_type":"markdown","id":"46403360","metadata":{"id":"46403360"},"source":["The code below is run for baseline model only by using only the first 500 games from the dataset. You might need to create a data generator to use complete dataset. Otherwise your RAM might not enough to store all (If you run the code on free version of Google Colab, it will crash above 500 game samples)."]},{"cell_type":"code","execution_count":8,"id":"a9bb0ab0","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13602,"status":"ok","timestamp":1697462756129,"user":{"displayName":"Ting Yang Lu (アズーサ)","userId":"17219646350901583040"},"user_tz":-480},"id":"a9bb0ab0","outputId":"efa515d8-1d57-4813-85df-1f148aadd157"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[[[ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   ...\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]]\n","\n","  [[ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   ...\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]]\n","\n","  [[ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   ...\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]]\n","\n","  ...\n","\n","  [[ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   ...\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]]\n","\n","  [[ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   ...\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]]\n","\n","  [[ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   ...\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]]]\n","\n","\n"," [[[ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   ...\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]]\n","\n","  [[ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   ...\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]]\n","\n","  [[ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   ...\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]]\n","\n","  ...\n","\n","  [[ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   ...\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]]\n","\n","  [[ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   ...\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]]\n","\n","  [[ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   ...\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]]]\n","\n","\n"," [[[ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   ...\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]]\n","\n","  [[ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   ...\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]]\n","\n","  [[ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   ...\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]]\n","\n","  ...\n","\n","  [[ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   ...\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]]\n","\n","  [[ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   ...\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]]\n","\n","  [[ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   ...\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]]]\n","\n","\n"," ...\n","\n","\n"," [[[ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   ...\n","   [ 1. -1.  0. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]]\n","\n","  [[ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   ...\n","   [ 1. -1.  0. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]]\n","\n","  [[ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 1. -1.  0. ...  0.  0.  0.]\n","   ...\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]]\n","\n","  ...\n","\n","  [[ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 1. -1.  0. ...  0.  0.  0.]\n","   ...\n","   [ 1. -1.  0. ...  0.  0.  0.]\n","   [-1. -1.  0. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]]\n","\n","  [[ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   ...\n","   [-1. -1.  0. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]]\n","\n","  [[ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   ...\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]]]\n","\n","\n"," [[[ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   ...\n","   [ 1. -1.  0. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]]\n","\n","  [[ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   ...\n","   [ 1. -1.  0. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]]\n","\n","  [[ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 1. -1.  0. ...  0.  0.  0.]\n","   ...\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]]\n","\n","  ...\n","\n","  [[ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 1. -1.  0. ...  0.  0.  0.]\n","   ...\n","   [ 1. -1.  0. ...  0.  0.  0.]\n","   [-1. -1.  0. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]]\n","\n","  [[ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   ...\n","   [-1. -1.  0. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]]\n","\n","  [[ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   ...\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]]]\n","\n","\n"," [[[ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   ...\n","   [ 1. -1.  0. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]]\n","\n","  [[ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   ...\n","   [ 1. -1.  0. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]]\n","\n","  [[ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 1. -1.  0. ...  0.  0.  0.]\n","   ...\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]]\n","\n","  ...\n","\n","  [[ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 1. -1.  0. ...  0.  0.  0.]\n","   ...\n","   [ 1. -1.  0. ...  0.  0.  0.]\n","   [-1. -1.  0. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]]\n","\n","  [[ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   ...\n","   [-1. -1.  0. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]]\n","\n","  [[ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   ...\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]\n","   [ 0.  0.  1. ...  0.  0.  0.]]]]\n"]}],"source":["import random\n","\n","games = random.sample(games, 500)\n","x = []\n","y = []\n","for game in games:\n","    moves_list = game.split(',')\n","    for count, move in enumerate(moves_list):\n","        x.append(prepare_input(moves_list[:count]))\n","        y.append(prepare_label(moves_list[count]))\n","x = np.array(x)\n","y = np.array(y)\n","print(x)"]},{"cell_type":"code","execution_count":9,"id":"5b2392a4","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1697297174338,"user":{"displayName":"Ting Yang Lu (アズーサ)","userId":"17219646350901583040"},"user_tz":-480},"id":"5b2392a4","outputId":"3a6ecd28-9609-4aca-c16e-80a379cc0a39"},"outputs":[{"data":{"text/plain":["(116236, 19, 19, 11)"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["x.shape"]},{"cell_type":"code","execution_count":10,"id":"73521b4f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1697297174338,"user":{"displayName":"Ting Yang Lu (アズーサ)","userId":"17219646350901583040"},"user_tz":-480},"id":"73521b4f","outputId":"c742ace0-7f80-49bb-dc79-9e4ddec4b6b2"},"outputs":[{"data":{"text/plain":["(116236,)"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["y.shape"]},{"cell_type":"code","execution_count":11,"id":"5510a7a6","metadata":{"executionInfo":{"elapsed":3407,"status":"ok","timestamp":1697462759535,"user":{"displayName":"Ting Yang Lu (アズーサ)","userId":"17219646350901583040"},"user_tz":-480},"id":"5510a7a6"},"outputs":[],"source":["y_one_hot = tf.one_hot(y, depth=19*19)"]},{"cell_type":"code","execution_count":12,"id":"90dfd8b2","metadata":{},"outputs":[{"data":{"text/plain":["<tf.Tensor: shape=(116236, 361), dtype=float32, numpy=\n","array([[0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       ...,\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["y_one_hot"]},{"cell_type":"markdown","id":"78b048ff","metadata":{"id":"78b048ff"},"source":["Dataset splitting: 90% Training, 10% validation"]},{"cell_type":"code","execution_count":13,"id":"3f594acb","metadata":{"executionInfo":{"elapsed":734,"status":"ok","timestamp":1697462760263,"user":{"displayName":"Ting Yang Lu (アズーサ)","userId":"17219646350901583040"},"user_tz":-480},"id":"3f594acb"},"outputs":[],"source":["x_train, x_val, y_train, y_val = train_test_split(x, y_one_hot.numpy(), test_size=0.05)"]},{"cell_type":"code","execution_count":null,"id":"3518e120","metadata":{},"outputs":[],"source":["from keras.models import Sequential\n","from keras.layers import Average\n","\n","def create_model1():\n","    inputs = Input(shape=(19, 19, 11))\n","    outputs = Conv2D(kernel_size=12, filters=64, padding='same', activation='relu', kernel_regularizer= regularizers.l2(0.003))(inputs)\n","    outputs = BatchNormalization()(outputs)\n","    outputs = Conv2D(kernel_size=7, filters=64, padding='same', activation='relu', kernel_regularizer= regularizers.l2(0.003))(outputs)\n","    outputs = BatchNormalization()(outputs)\n","    outputs = Conv2D(kernel_size=5, filters=64, padding='same', activation='relu', kernel_regularizer= regularizers.l2(0.003))(outputs)\n","    outputs = BatchNormalization()(outputs)\n","    outputs = Conv2D(kernel_size=5, filters=32, padding='same', activation='relu', kernel_regularizer= regularizers.l2(0.003))(outputs)\n","    outputs = BatchNormalization()(outputs)\n","    outputs = Conv2D(kernel_size=3, filters=32, padding='same', activation='relu', kernel_regularizer= regularizers.l2(0.003))(outputs)\n","    outputs = BatchNormalization()(outputs)\n","    outputs = Conv2D(kernel_size=3, filters=32, padding='same', activation='relu',kernel_regularizer= regularizers.l2(0.003))(outputs)\n","    outputs = BatchNormalization()(outputs)\n","    outputs = Conv2D(kernel_size=3, filters=32, padding='same', activation='relu',kernel_regularizer= regularizers.l2(0.003))(outputs)\n","    outputs = BatchNormalization()(outputs)\n","    outputs = Conv2D(kernel_size=16, filters=32, padding='same', activation='relu',kernel_regularizer= regularizers.l2(0.003))(outputs)\n","    outputs = BatchNormalization()(outputs)\n","    outputs = Conv2D(kernel_size=16, filters=32, padding='same', activation='relu',kernel_regularizer= regularizers.l2(0.003))(outputs)\n","    outputs = BatchNormalization()(outputs)\n","    outputs = Conv2D(kernel_size=16, filters=32, padding='same', activation='relu',kernel_regularizer= regularizers.l2(0.003))(outputs)\n","    outputs = BatchNormalization()(outputs)\n","    outputs = Conv2D(kernel_size=16, filters=1, padding='same', activation='relu',kernel_regularizer= regularizers.l2(0.003))(outputs)\n","    outputs = BatchNormalization()(outputs)\n","\n","    outputs = Flatten()(outputs)\n","    outputs = Softmax()(outputs)\n","    outputs = Dropout(0.05)(outputs)\n","\n","    model = Model(inputs, outputs)\n","\n","    opt = Adam(learning_rate=0.001)\n","    model.compile(optimizer=opt,\n","                  loss='categorical_crossentropy',\n","                  metrics=['accuracy'])\n","    return model\n","\n","\n","# Create an ensemble model using Average and Lambda layers\n","models = [create_model1() for i in range(5)]\n","model_input = Input(shape=(19, 19, 11))\n","model_outputs = [model(model_input) for model in models]\n","ensemble_output = Average()(model_outputs)\n","ensemble_model = Model(inputs=model_input, outputs=ensemble_output)\n","ensemble_model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"id":"5f4f8bc9","metadata":{},"outputs":[],"source":["from keras.models import Sequential\n","from keras.layers import Average\n","\n","def create_model1():\n","    inputs = Input(shape=(19, 19, 11))\n","\n","    x = Conv2D(kernel_size=3, filters=64, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.001))(inputs)\n","    x = BatchNormalization()(x)\n","\n","    x = Conv2D(kernel_size=3, filters=128, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n","    x = BatchNormalization()(x)\n"," \n","\n","    x = Flatten()(x)\n","    x = Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n","    x = BatchNormalization()(x)\n","    x = Dropout(0.5)(x)\n","\n","    x = Dense(361, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n","    x = BatchNormalization()(x)\n","    x = Dropout(0.5)(x)\n","\n","    outputs = Dense(361, activation='softmax')(x)\n","\n","    model = Model(inputs, outputs)\n","\n","    opt = Adam(learning_rate=0.0005)\n","    model.compile(optimizer=opt,\n","                  loss='categorical_crossentropy',\n","                  metrics=['accuracy'])\n","    return model\n","\n","\n","# Create an ensemble model using Average and Lambda layers\n","models = [create_model1() for i in range(5)]\n","model_input = Input(shape=(19, 19, 11))\n","model_outputs = [model(model_input) for model in models]\n","ensemble_output = Average()(model_outputs)\n","ensemble_model = Model(inputs=model_input, outputs=ensemble_output)\n","ensemble_model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"]},{"cell_type":"markdown","id":"7b9c5de9","metadata":{"id":"7b9c5de9"},"source":["# Training\n","\n","### Simple DCNN Model:"]},{"cell_type":"code","execution_count":null,"id":"208834da","metadata":{"executionInfo":{"elapsed":273,"status":"ok","timestamp":1697462774176,"user":{"displayName":"Ting Yang Lu (アズーサ)","userId":"17219646350901583040"},"user_tz":-480},"id":"208834da"},"outputs":[],"source":["def create_model6():\n","    inputs = Input(shape=(19, 19, 11))\n","\n","    x = Conv2D(kernel_size=3, filters=64, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.001))(inputs)\n","    x = BatchNormalization()(x)\n","\n","    x = Conv2D(kernel_size=3, filters=128, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n","    x = BatchNormalization()(x)\n","    x = Conv2D(kernel_size=3, filters=256, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n","    x = BatchNormalization()(x)\n","\n","    x = Conv2D(kernel_size=3, filters=512, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n","    x = BatchNormalization()(x)\n","\n","    x = Flatten()(x)\n","    x = Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n","    x = BatchNormalization()(x)\n","    x = Dropout(0.5)(x)\n","\n","    x = Dense(361, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n","    x = BatchNormalization()(x)\n","    x = Dropout(0.5)(x)\n","\n","    outputs = Dense(361, activation='softmax')(x)\n","\n","    model = Model(inputs, outputs)\n","\n","    opt = Adam(learning_rate=0.0005)\n","    model.compile(optimizer=opt,\n","                  loss='categorical_crossentropy',\n","                  metrics=['accuracy'])\n","    return model\n","\n"]},{"cell_type":"code","execution_count":null,"id":"b021c54a","metadata":{},"outputs":[],"source":["from keras.models import Sequential\n","def create_model5():\n","    model = Sequential()\n","\n","    model.add(Conv2D(kernel_size=3, filters=64, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.001), input_shape=(19, 19, 11)))\n","    model.add(BatchNormalization())\n","\n","    model.add(Conv2D(kernel_size=3, filters=128, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n","    model.add(BatchNormalization())\n","    model.add(MaxPooling2D(pool_size=2))\n","    model.add(Dropout(0.25))\n","\n","    model.add(Conv2D(kernel_size=3, filters=256, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n","    model.add(BatchNormalization())\n","    model.add(MaxPooling2D(pool_size=2))\n","    model.add(Dropout(0.25))\n","\n","    model.add(Conv2D(kernel_size=3, filters=512, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n","    model.add(BatchNormalization())\n","    model.add(MaxPooling2D(pool_size=2))\n","    model.add(Dropout(0.25))\n","\n","    model.add(Flatten())\n","    model.add(Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n","    model.add(BatchNormalization())\n","    model.add(Dropout(0.5))\n","\n","    model.add(Dense(361, activation='relu', kernel_regularizer=regularizers.l2(0.001)))\n","    model.add(BatchNormalization())\n","    model.add(Dropout(0.5))\n","\n","    model.add(Dense(361, activation='softmax'))\n","\n","    opt = Adam(learning_rate=0.0005)\n","    model.compile(optimizer=opt,\n","                  loss='categorical_crossentropy',\n","                  metrics=['accuracy'])\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"id":"868325ac","metadata":{},"outputs":[],"source":["def create_model2():\n","    inputs = Input(shape=(19, 19, 11))\n","    outputs = Conv2D(kernel_size=12, filters=64, padding='same', activation='relu', kernel_regularizer= regularizers.l2(0.003))(inputs)\n","    outputs = BatchNormalization()(outputs)\n","    outputs = Conv2D(kernel_size=7, filters=64, padding='same', activation='relu', kernel_regularizer= regularizers.l2(0.003))(outputs)\n","    outputs = BatchNormalization()(outputs)\n","    outputs = Conv2D(kernel_size=5, filters=64, padding='same', activation='relu', kernel_regularizer= regularizers.l2(0.003))(outputs)\n","    outputs = BatchNormalization()(outputs)\n","    outputs = Conv2D(kernel_size=5, filters=32, padding='same', activation='relu', kernel_regularizer= regularizers.l2(0.003))(outputs)\n","    outputs = BatchNormalization()(outputs)\n","    outputs = Conv2D(kernel_size=3, filters=32, padding='same', activation='relu', kernel_regularizer= regularizers.l2(0.003))(outputs)\n","    outputs = BatchNormalization()(outputs)\n","    outputs = Conv2D(kernel_size=3, filters=32, padding='same', activation='relu',kernel_regularizer= regularizers.l2(0.003))(outputs)\n","    outputs = BatchNormalization()(outputs)\n","    outputs = Conv2D(kernel_size=3, filters=32, padding='same', activation='relu',kernel_regularizer= regularizers.l2(0.003))(outputs)\n","    outputs = BatchNormalization()(outputs)\n","    outputs = Conv2D(kernel_size=16, filters=32, padding='same', activation='relu',kernel_regularizer= regularizers.l2(0.003))(outputs)\n","    outputs = BatchNormalization()(outputs)\n","    outputs = Conv2D(kernel_size=16, filters=32, padding='same', activation='relu',kernel_regularizer= regularizers.l2(0.003))(outputs)\n","    outputs = BatchNormalization()(outputs)\n","    outputs = Conv2D(kernel_size=16, filters=32, padding='same', activation='relu',kernel_regularizer= regularizers.l2(0.003))(outputs)\n","    outputs = BatchNormalization()(outputs)\n","    outputs = Conv2D(kernel_size=16, filters=1, padding='same', activation='relu',kernel_regularizer= regularizers.l2(0.003))(outputs)\n","    outputs = BatchNormalization()(outputs)\n","\n","    outputs = Flatten()(outputs)\n","    outputs = Softmax()(outputs)\n","    outputs = Dropout(0.05)(outputs)\n","\n","    model = Model(inputs, outputs)\n","\n","    opt = Adam(learning_rate=0.001)\n","    model.compile(optimizer=opt,\n","                  loss='categorical_crossentropy',\n","                  metrics=['accuracy'])\n","    return model"]},{"cell_type":"code","execution_count":null,"id":"2a66e90d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":655,"status":"ok","timestamp":1697462776359,"user":{"displayName":"Ting Yang Lu (アズーサ)","userId":"17219646350901583040"},"user_tz":-480},"id":"2a66e90d","outputId":"90125a7f-eb89-4571-abb8-a9c1098b6580"},"outputs":[],"source":["model = create_model()\n","model2 = create_model2()\n","model.summary()\n","model2.summary()"]},{"cell_type":"code","execution_count":null,"id":"e36cec7b","metadata":{},"outputs":[],"source":["ensemble_model.summary()"]},{"cell_type":"code","execution_count":null,"id":"f0133467","metadata":{},"outputs":[],"source":["model6 = create_model6()\n","model6.summary()"]},{"cell_type":"code","execution_count":14,"id":"67ec4d6e","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n","1 Physical GPUs, 1 Logical GPU\n"]}],"source":["import tensorflow as tf\n","gpus = tf.config.list_physical_devices('GPU')\n","print(gpus)\n","if gpus:\n","    \n","    # Restrict TensorFlow to only use the first GPU\n","    try:\n","        tf.config.set_visible_devices(gpus[0], 'GPU')\n","        logical_gpus = tf.config.list_logical_devices('GPU')\n","        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n","    except RuntimeError as e:\n","        # Visible devices must be set before GPUs have been initialized\n","        print(e)"]},{"cell_type":"code","execution_count":null,"id":"4a4d7f1c","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":206582,"status":"ok","timestamp":1697462985354,"user":{"displayName":"Ting Yang Lu (アズーサ)","userId":"17219646350901583040"},"user_tz":-480},"id":"4a4d7f1c","outputId":"b9a9dad4-f1c2-4c4f-8707-1c8de3dc48d3"},"outputs":[],"source":["def data_generator(x, y, batch_size):\n","    while True:\n","        for batch_start in range(0, len(x), batch_size):\n","            batch_end = batch_start + batch_size\n","            x_batch = x[batch_start:batch_end]\n","            y_batch = y[batch_start:batch_end]\n","            yield x_batch, y_batch\n","\n","# 創建生成器\n","batch_size = 64  # 設置合適的批次大小\n","train_generator = data_generator(x_train, y_train, batch_size)\n","val_generator = data_generator(x_val, y_val, batch_size)\n","\n","'''\n","history = model.fit(\n","    train_generator,\n","    steps_per_epoch=len(x_train)//batch_size,\n","    epochs=40,\n","    validation_data=val_generator,\n","    validation_steps=len(x_val)//batch_size\n",")\n","'''\n","history = ensemble_model.fit(\n","    train_generator,\n","    steps_per_epoch=len(x_train)//batch_size,\n","    epochs=20,\n","    validation_data=val_generator,\n","    validation_steps=len(x_val)//batch_size\n",")"]},{"cell_type":"code","execution_count":null,"id":"04296ae9","metadata":{},"outputs":[],"source":["history = ensemble_model.fit(\n","    x = x_train,\n","    y = y_train,\n","    batch_size = 256,\n","    epochs = 40,\n","    validation_data=(x_val, y_val),\n",")"]},{"cell_type":"code","execution_count":null,"id":"2aaddf0b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":756,"status":"ok","timestamp":1697463024840,"user":{"displayName":"Ting Yang Lu (アズーサ)","userId":"17219646350901583040"},"user_tz":-480},"id":"2aaddf0b","outputId":"1b7dbf04-1ee0-4bb1-e2bb-33785bdea42c"},"outputs":[],"source":["model6.save('C:/Users/Tidera/Downloads/drive-download-20231017T013717Z-001/model_kyu_tutorial6.h5')"]},{"cell_type":"code","execution_count":null,"id":"5b85efd6","metadata":{},"outputs":[],"source":["def data_generator(x, y, batch_size):\n","    while True:\n","        for batch_start in range(0, len(x), batch_size):\n","            batch_end = batch_start + batch_size\n","            x_batch = x[batch_start:batch_end]\n","            y_batch = y[batch_start:batch_end]\n","            yield x_batch, y_batch\n","\n","# 創建生成器\n","batch_size = 256  # 設置合適的批次大小\n","train_generator = data_generator(x_train, y_train, batch_size)\n","val_generator = data_generator(x_val, y_val, batch_size)\n","\n","history = model6.fit(\n","    train_generator,\n","    steps_per_epoch=len(x_train)//batch_size,\n","    epochs=40,\n","    validation_data=val_generator,\n","    validation_steps=len(x_val)//batch_size\n",")"]},{"cell_type":"code","execution_count":null,"id":"a04cf5ac","metadata":{},"outputs":[],"source":["model6.save('C:/Users/Tidera/Downloads/drive-download-20231017T013717Z-001/model_kyu_tutorial6.h5')"]},{"cell_type":"code","execution_count":null,"id":"ce0a189e","metadata":{},"outputs":[],"source":["def data_generator(x, y, batch_size):\n","    while True:\n","        for batch_start in range(0, len(x), batch_size):\n","            batch_end = batch_start + batch_size\n","            x_batch = x[batch_start:batch_end]\n","            y_batch = y[batch_start:batch_end]\n","            yield x_batch, y_batch\n","\n","# 創建生成器\n","batch_size = 256  # 設置合適的批次大小\n","train_generator = data_generator(x_train, y_train, batch_size)\n","val_generator = data_generator(x_val, y_val, batch_size)\n","\n","history5 = model5.fit(\n","    train_generator,\n","    steps_per_epoch=len(x_train)//batch_size,\n","    epochs=40,\n","    validation_data=val_generator,\n","    validation_steps=len(x_val)//batch_size\n"," )"]},{"cell_type":"code","execution_count":null,"id":"fd29c763","metadata":{},"outputs":[],"source":["model5.save('C:/Users/Tidera/Downloads/drive-download-20231017T013717Z-001/model_kyu_tutorial5.h5')"]},{"cell_type":"code","execution_count":15,"id":"ed2a22a9","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["182/182 [==============================] - 3s 4ms/step\n","182/182 [==============================] - 3s 16ms/step\n","182/182 [==============================] - 1s 5ms/step\n","182/182 [==============================] - 0s 2ms/step\n","182/182 [==============================] - 1s 5ms/step\n","Ensemble Accuracy: 0.4377150722642808\n"]}],"source":["from keras.models import load_model\n","from sklearn.metrics import accuracy_score\n","import numpy as np\n","\n","# 載入模型\n","\n","loaded_model_2 = load_model('C:/Users/Tidera/Downloads/drive-download-20231017T013717Z-001/model_kyu_tutorial2.h5')\n","loaded_model_3 = load_model('C:/Users/Tidera/Downloads/drive-download-20231017T013717Z-001/model_kyu_tutorial3.h5')\n","loaded_model_4 = load_model('C:/Users/Tidera/Downloads/drive-download-20231017T013717Z-001/model_kyu_tutorial4.h5')\n","loaded_model_5 = load_model('C:/Users/Tidera/Downloads/drive-download-20231017T013717Z-001/model_kyu_tutorial5.h5')\n","loaded_model_6 = load_model('C:/Users/Tidera/Downloads/drive-download-20231017T013717Z-001/model_kyu_tutorial6.h5')\n","\n","# 使用載入的模型進行預測\n","\n","pred_probs_model2 = loaded_model_2.predict(x_val)\n","pred_probs_model3 = loaded_model_3.predict(x_val)\n","pred_probs_model4 = loaded_model_4.predict(x_val)\n","pred_probs_model5 = loaded_model_5.predict(x_val)\n","pred_probs_model6 = loaded_model_6.predict(x_val)\n","\n","# 投票法 Ensemble\n","def ensemble_voting(pred_probs_list):\n","    final_predictions = []\n","    for i in range(len(pred_probs_list[0])):\n","        votes = [pred_probs[i].argmax() for pred_probs in pred_probs_list]\n","        #votes = [pred_probs[i] for pred_probs in pred_probs_list]\n","        final_predictions.append([max(set(votes), key=votes.count)])\n","        #average_vote = np.mean(votes, axis=0)\n","        #final_predictions.append(average_vote)\n","      \n","    return final_predictions\n","\n","# 將模型預測機率組成的列表傳入投票法\n","final_predictions = ensemble_voting([ pred_probs_model2, pred_probs_model3, pred_probs_model4, pred_probs_model5, pred_probs_model6])\n","#print(final_predictions)\n","# 將 y_val 轉換為單一標籤形式\n","y_val_single_label = y_val.argmax(axis=1)\n","#print(y_val_single_label)\n","# 計算 Ensemble 後的準確率\n","accuracy = accuracy_score(y_val_single_label, final_predictions)\n","print(\"Ensemble Accuracy:\", accuracy)"]},{"cell_type":"markdown","id":"484be28d","metadata":{"id":"484be28d"},"source":["# Testing\n","\n","**PublicUpload.csv** must be in the following form:\n","```\n","KTPU0000000001,id,qr,pq,pd,ab\n","KTPU0000000002,ao,ab,ha,ff,qd\n","KTPU0000000003,qd,gd,fh,ed,fa\n","KTPU0000000004,pr,ba,dq,hh,aj\n","KTPU0000000005,ph,jh,af,df,gj\n","```\n","\n","- Column 1: Game ID\n","- Column 2: Predicted Moves, up to 5 predictions for each game\n","\n","The code block below is to use **kyu_test_public.csv** to predict and save the results in required form. It generates the best 5 predictions for each sample and convert them to character coordinates."]},{"cell_type":"code","execution_count":null,"id":"ad5e2bb1","metadata":{"executionInfo":{"elapsed":266,"status":"ok","timestamp":1697463026814,"user":{"displayName":"Ting Yang Lu (アズーサ)","userId":"17219646350901583040"},"user_tz":-480},"id":"ad5e2bb1"},"outputs":[],"source":["def number_to_char(number):\n","    number_1, number_2 = divmod(number, 19)\n","    return chartonumbers[number_1] + chartonumbers[number_2]\n","\n","def top_5_preds_with_chars(predictions):\n","    resulting_preds_numbers = [np.flip(np.argpartition(prediction, -5)[-5:]) for prediction in predictions]\n","    resulting_preds_chars = np.vectorize(number_to_char)(resulting_preds_numbers)\n","    return resulting_preds_chars"]},{"cell_type":"code","execution_count":null,"id":"18e776de","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3942,"status":"ok","timestamp":1697463031755,"user":{"displayName":"Ting Yang Lu (アズーサ)","userId":"17219646350901583040"},"user_tz":-480},"id":"18e776de","outputId":"00f0c92a-567b-428b-c492-4de92ad668b1"},"outputs":[],"source":["from keras.models import load_model\n","\n","# 投票法 Ensemble\n","def ensemble_voting(pred_probs_list):\n","    final_predictions = []\n","    for i in range(len(pred_probs_list[0])):\n","        #votes = [pred_probs[i].argmax() for pred_probs in pred_probs_list]\n","        votes = [pred_probs[i] for pred_probs in pred_probs_list]\n","        #final_predictions.append([max(set(votes), key=votes.count)])\n","        average_vote = np.mean(votes, axis=0)\n","        final_predictions.append(average_vote)\n","      \n","    return final_predictions\n","\n","df = open('C:/Users/Tidera/Downloads/drive-download-20231017T013717Z-001/kyu_test_public.csv').read().splitlines()\n","games_id = [i.split(',',2)[0] for i in df]\n","games = [i.split(',',2)[-1] for i in df]\n","\n","x_testing = []\n","\n","for game in games:\n","    moves_list = game.split(',')\n","    x_testing.append(prepare_input(moves_list))\n","\n","x_testing = np.array(x_testing)\n","\n","# 載入模型\n","loaded_model_1 = load_model('C:/Users/Tidera/Downloads/drive-download-20231017T013717Z-001/model_kyu_tutorial.h5')\n","loaded_model_2 = load_model('C:/Users/Tidera/Downloads/drive-download-20231017T013717Z-001/model_kyu_tutorial2.h5')\n","loaded_model_3 = load_model('C:/Users/Tidera/Downloads/drive-download-20231017T013717Z-001/model_kyu_tutorial3.h5')\n","loaded_model_4 = load_model('C:/Users/Tidera/Downloads/drive-download-20231017T013717Z-001/model_kyu_tutorial4.h5')\n","loaded_model_5 = load_model('C:/Users/Tidera/Downloads/drive-download-20231017T013717Z-001/model_kyu_tutorial5.h5')\n","loaded_model_6 = load_model('C:/Users/Tidera/Downloads/drive-download-20231017T013717Z-001/model_kyu_tutorial6.h5')\n","\n","pred_probs_model1 = loaded_model_1.predict(x_testing)\n","pred_probs_model2 = loaded_model_2.predict(x_testing)\n","pred_probs_model3 = loaded_model_3.predict(x_testing)\n","pred_probs_model4 = loaded_model_4.predict(x_testing)\n","pred_probs_model5 = loaded_model_5.predict(x_testing)\n","pred_probs_model6 = loaded_model_6.predict(x_testing)\n","\n","\n","predictions = final_predictions = ensemble_voting([pred_probs_model1, pred_probs_model2, pred_probs_model3, pred_probs_model4, pred_probs_model5, pred_probs_model6])\n","prediction_chars = top_5_preds_with_chars(predictions)\n","\n","\n","# Save results to PublicUpload.csv\n","with open('C:/Users/Tidera/Downloads/drive-download-20231017T013717Z-001/PublicUpload1128-3.csv','a') as f:\n","    for index in range(len(prediction_chars)):\n","        answer_row = games_id[index] + ',' + ','.join(prediction_chars[index]) + '\\n'\n","        f.write(answer_row)"]},{"cell_type":"markdown","id":"2f7fafaa","metadata":{"id":"2f7fafaa"},"source":["# End of Tutorial\n","\n","You are free to use more modern NN architectures, a better pre-processing, feature extraction methods to achieve much better accuracy!"]},{"cell_type":"code","execution_count":null,"id":"5a6c5af6","metadata":{},"outputs":[],"source":["df = open('C:/Users/Tidera/Downloads/drive-download-20231017T013717Z-001/kyu_test_private.csv').read().splitlines()\n","games_id = [i.split(',',2)[0] for i in df]\n","games = [i.split(',',2)[-1] for i in df]\n","\n","x_testing = []\n","\n","for game in games:\n","    moves_list = game.split(',')\n","    x_testing.append(prepare_input(moves_list))\n","\n","x_testing = np.array(x_testing)\n","\n","pred_probs_model1 = loaded_model_1.predict(x_testing)\n","pred_probs_model2 = loaded_model_2.predict(x_testing)\n","pred_probs_model3 = loaded_model_3.predict(x_testing)\n","pred_probs_model4 = loaded_model_4.predict(x_testing)\n","pred_probs_model5 = loaded_model_5.predict(x_testing)\n","pred_probs_model6 = loaded_model_6.predict(x_testing)\n","\n","predictions = final_predictions = ensemble_voting([pred_probs_model1, pred_probs_model2, pred_probs_model3, pred_probs_model4])\n","prediction_chars = top_5_preds_with_chars(predictions)\n","\n","\n","# Save results to PublicUpload.csv\n","with open('C:/Users/Tidera/Downloads/drive-download-20231017T013717Z-001/PublicUpload1128-3.csv','a') as f:\n","    for index in range(len(prediction_chars)):\n","        answer_row = games_id[index] + ',' + ','.join(prediction_chars[index]) + '\\n'\n","        f.write(answer_row)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.11"}},"nbformat":4,"nbformat_minor":5}
